{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 一、数据处理",
   "id": "7bf98786df2de87a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. 读取数据",
   "id": "cd4c3c3fbf3986ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:22:50.808634Z",
     "start_time": "2025-10-15T06:22:48.616744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0,\n",
    " delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# 查看数据\n",
    "print(train[\"review\"].size)\n",
    "print(test[\"review\"].size)\n",
    "print(unlabeled_train[\"review\"].size)"
   ],
   "id": "453a55ca98351d73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "50000\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 数据清洗",
   "id": "ee5c5093fa811797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:22:55.737138Z",
     "start_time": "2025-10-15T06:22:53.613548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def review_to_wordlist(raw_review, remove_stopwords=False):\n",
    "    # 删除HTML标签\n",
    "    review_text = BeautifulSoup(raw_review, \"html.parser\").get_text()\n",
    "    # 去除标点和数字\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # 小写、分词\n",
    "    review_cleaned = letters_only.lower().split()\n",
    "    # 训练Word2Vec最好不去除停用词\n",
    "    if remove_stopwords:\n",
    "        # 去除停用词\n",
    "        # 转为集合提高效率\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        review_cleaned = [w for w in review_cleaned if not w in stop_words]\n",
    "    return review_cleaned"
   ],
   "id": "2652d9cc746eb855",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. 划分句子",
   "id": "7acbae768c9572ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:23:46.798544Z",
     "start_time": "2025-10-15T06:22:59.048869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# 加载 punkt\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 切分句子\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            # 将词列表加入句子列表\n",
    "            sentences.append(review_to_wordlist(sentence, remove_stopwords = remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "# 获取输入数据\n",
    "sentences = []\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer,\n",
    "                                     remove_stopwords=False)\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer,\n",
    "                                     remove_stopwords=False)\n",
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(sentences[1])"
   ],
   "id": "e19830b18cd16a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25880\\1221155664.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(raw_review, \"html.parser\").get_text()\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25880\\1221155664.py:6: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  review_text = BeautifulSoup(raw_review, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796172\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 二、模型训练",
   "id": "33ec0c99f147d39e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T03:09:50.654203Z",
     "start_time": "2025-10-15T03:07:58.929077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 设置模型参数\n",
    "num_features = 300\n",
    "min_word_count = 40  # 保留出现次数>=40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "down_sampling = 1e-3\n",
    "\n",
    "# 默认使用skip-gram模型\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,  vector_size=num_features, min_count=min_word_count, window=context, sample=down_sampling)\n",
    "\n",
    "# 保存模型\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ],
   "id": "3e3c3836d980df8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 11:07:58,933 : INFO : collecting all words and their counts\n",
      "2025-10-15 11:07:58,934 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-10-15 11:07:58,966 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2025-10-15 11:07:58,996 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
      "2025-10-15 11:07:59,022 : INFO : PROGRESS: at sentence #30000, processed 670858 words, keeping 30027 word types\n",
      "2025-10-15 11:07:59,052 : INFO : PROGRESS: at sentence #40000, processed 896840 words, keeping 34335 word types\n",
      "2025-10-15 11:07:59,092 : INFO : PROGRESS: at sentence #50000, processed 1116081 words, keeping 37751 word types\n",
      "2025-10-15 11:07:59,124 : INFO : PROGRESS: at sentence #60000, processed 1337543 words, keeping 40711 word types\n",
      "2025-10-15 11:07:59,156 : INFO : PROGRESS: at sentence #70000, processed 1560306 words, keeping 43311 word types\n",
      "2025-10-15 11:07:59,196 : INFO : PROGRESS: at sentence #80000, processed 1779515 words, keeping 45707 word types\n",
      "2025-10-15 11:07:59,230 : INFO : PROGRESS: at sentence #90000, processed 2003713 words, keeping 48121 word types\n",
      "2025-10-15 11:07:59,264 : INFO : PROGRESS: at sentence #100000, processed 2225464 words, keeping 50190 word types\n",
      "2025-10-15 11:07:59,295 : INFO : PROGRESS: at sentence #110000, processed 2444322 words, keeping 52058 word types\n",
      "2025-10-15 11:07:59,321 : INFO : PROGRESS: at sentence #120000, processed 2666487 words, keeping 54098 word types\n",
      "2025-10-15 11:07:59,345 : INFO : PROGRESS: at sentence #130000, processed 2892314 words, keeping 55837 word types\n",
      "2025-10-15 11:07:59,370 : INFO : PROGRESS: at sentence #140000, processed 3104795 words, keeping 57324 word types\n",
      "2025-10-15 11:07:59,397 : INFO : PROGRESS: at sentence #150000, processed 3330431 words, keeping 59045 word types\n",
      "2025-10-15 11:07:59,422 : INFO : PROGRESS: at sentence #160000, processed 3552465 words, keeping 60581 word types\n",
      "2025-10-15 11:07:59,447 : INFO : PROGRESS: at sentence #170000, processed 3776047 words, keeping 62050 word types\n",
      "2025-10-15 11:07:59,491 : INFO : PROGRESS: at sentence #180000, processed 3996236 words, keeping 63483 word types\n",
      "2025-10-15 11:07:59,527 : INFO : PROGRESS: at sentence #190000, processed 4221287 words, keeping 64775 word types\n",
      "2025-10-15 11:07:59,568 : INFO : PROGRESS: at sentence #200000, processed 4445972 words, keeping 66070 word types\n",
      "2025-10-15 11:07:59,597 : INFO : PROGRESS: at sentence #210000, processed 4666510 words, keeping 67367 word types\n",
      "2025-10-15 11:07:59,644 : INFO : PROGRESS: at sentence #220000, processed 4892036 words, keeping 68686 word types\n",
      "2025-10-15 11:07:59,693 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2025-10-15 11:07:59,727 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2025-10-15 11:07:59,774 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2025-10-15 11:07:59,813 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2025-10-15 11:07:59,843 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2025-10-15 11:07:59,879 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2025-10-15 11:07:59,916 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2025-10-15 11:07:59,944 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2025-10-15 11:07:59,968 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2025-10-15 11:07:59,994 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2025-10-15 11:08:00,023 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2025-10-15 11:08:00,058 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2025-10-15 11:08:00,107 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2025-10-15 11:08:00,135 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2025-10-15 11:08:00,166 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2025-10-15 11:08:00,195 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2025-10-15 11:08:00,231 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2025-10-15 11:08:00,262 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2025-10-15 11:08:00,296 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2025-10-15 11:08:00,331 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2025-10-15 11:08:00,365 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2025-10-15 11:08:00,389 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2025-10-15 11:08:00,417 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2025-10-15 11:08:00,454 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2025-10-15 11:08:00,481 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2025-10-15 11:08:00,512 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2025-10-15 11:08:00,544 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2025-10-15 11:08:00,577 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2025-10-15 11:08:00,605 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2025-10-15 11:08:00,629 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2025-10-15 11:08:00,655 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2025-10-15 11:08:00,683 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2025-10-15 11:08:00,711 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2025-10-15 11:08:00,737 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2025-10-15 11:08:00,766 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2025-10-15 11:08:00,809 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2025-10-15 11:08:00,844 : INFO : PROGRESS: at sentence #590000, processed 13184324 words, keeping 108469 word types\n",
      "2025-10-15 11:08:00,879 : INFO : PROGRESS: at sentence #600000, processed 13406550 words, keeping 109190 word types\n",
      "2025-10-15 11:08:00,907 : INFO : PROGRESS: at sentence #610000, processed 13628197 words, keeping 110056 word types\n",
      "2025-10-15 11:08:00,934 : INFO : PROGRESS: at sentence #620000, processed 13852587 words, keeping 110806 word types\n",
      "2025-10-15 11:08:00,969 : INFO : PROGRESS: at sentence #630000, processed 14075900 words, keeping 111574 word types\n",
      "2025-10-15 11:08:01,001 : INFO : PROGRESS: at sentence #640000, processed 14298045 words, keeping 112387 word types\n",
      "2025-10-15 11:08:01,032 : INFO : PROGRESS: at sentence #650000, processed 14522873 words, keeping 113152 word types\n",
      "2025-10-15 11:08:01,064 : INFO : PROGRESS: at sentence #660000, processed 14745444 words, keeping 113891 word types\n",
      "2025-10-15 11:08:01,101 : INFO : PROGRESS: at sentence #670000, processed 14970568 words, keeping 114614 word types\n",
      "2025-10-15 11:08:01,131 : INFO : PROGRESS: at sentence #680000, processed 15194624 words, keeping 115332 word types\n",
      "2025-10-15 11:08:01,160 : INFO : PROGRESS: at sentence #690000, processed 15416772 words, keeping 116100 word types\n",
      "2025-10-15 11:08:01,194 : INFO : PROGRESS: at sentence #700000, processed 15645694 words, keeping 116903 word types\n",
      "2025-10-15 11:08:01,220 : INFO : PROGRESS: at sentence #710000, processed 15865814 words, keeping 117542 word types\n",
      "2025-10-15 11:08:01,248 : INFO : PROGRESS: at sentence #720000, processed 16093341 words, keeping 118184 word types\n",
      "2025-10-15 11:08:01,275 : INFO : PROGRESS: at sentence #730000, processed 16316786 words, keeping 118913 word types\n",
      "2025-10-15 11:08:01,310 : INFO : PROGRESS: at sentence #740000, processed 16539145 words, keeping 119619 word types\n",
      "2025-10-15 11:08:01,338 : INFO : PROGRESS: at sentence #750000, processed 16758550 words, keeping 120265 word types\n",
      "2025-10-15 11:08:01,364 : INFO : PROGRESS: at sentence #760000, processed 16977109 words, keeping 120889 word types\n",
      "2025-10-15 11:08:01,392 : INFO : PROGRESS: at sentence #770000, processed 17203257 words, keeping 121657 word types\n",
      "2025-10-15 11:08:01,422 : INFO : PROGRESS: at sentence #780000, processed 17432842 words, keeping 122359 word types\n",
      "2025-10-15 11:08:01,449 : INFO : PROGRESS: at sentence #790000, processed 17660149 words, keeping 123034 word types\n",
      "2025-10-15 11:08:01,473 : INFO : collected 123505 word types from a corpus of 17798268 raw words and 796172 sentences\n",
      "2025-10-15 11:08:01,474 : INFO : Creating a fresh vocabulary\n",
      "2025-10-15 11:08:01,546 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123505, drops 107015)', 'datetime': '2025-10-15T11:08:01.546742', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-10-15 11:08:01,547 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239122 word corpus (96.86% of original 17798268, drops 559146)', 'datetime': '2025-10-15T11:08:01.547742', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-10-15 11:08:01,599 : INFO : deleting the raw counts dictionary of 123505 items\n",
      "2025-10-15 11:08:01,605 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2025-10-15 11:08:01,608 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749795.99803372 word corpus (74.0%% of prior 17239122)', 'datetime': '2025-10-15T11:08:01.608802', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-10-15 11:08:01,686 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2025-10-15 11:08:01,690 : INFO : resetting layer weights\n",
      "2025-10-15 11:08:01,705 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-15T11:08:01.705860', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-10-15 11:08:01,707 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-10-15T11:08:01.707857', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-10-15 11:08:02,731 : INFO : EPOCH 0 - PROGRESS: at 6.45% examples, 807818 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:03,736 : INFO : EPOCH 0 - PROGRESS: at 12.87% examples, 806053 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:04,741 : INFO : EPOCH 0 - PROGRESS: at 19.33% examples, 807813 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:05,744 : INFO : EPOCH 0 - PROGRESS: at 26.18% examples, 823185 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:06,747 : INFO : EPOCH 0 - PROGRESS: at 32.86% examples, 826634 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:07,764 : INFO : EPOCH 0 - PROGRESS: at 39.43% examples, 825969 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:08,775 : INFO : EPOCH 0 - PROGRESS: at 46.06% examples, 828354 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:09,784 : INFO : EPOCH 0 - PROGRESS: at 52.04% examples, 819601 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:10,784 : INFO : EPOCH 0 - PROGRESS: at 58.01% examples, 814239 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:11,785 : INFO : EPOCH 0 - PROGRESS: at 64.11% examples, 810776 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:12,790 : INFO : EPOCH 0 - PROGRESS: at 70.28% examples, 808259 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:13,815 : INFO : EPOCH 0 - PROGRESS: at 75.76% examples, 797746 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:14,819 : INFO : EPOCH 0 - PROGRESS: at 80.48% examples, 782459 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:15,821 : INFO : EPOCH 0 - PROGRESS: at 84.94% examples, 767435 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:16,849 : INFO : EPOCH 0 - PROGRESS: at 89.37% examples, 752643 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:17,862 : INFO : EPOCH 0 - PROGRESS: at 93.97% examples, 741793 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:18,864 : INFO : EPOCH 0 - PROGRESS: at 98.71% examples, 733886 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:19,136 : INFO : EPOCH 0: training on 17798268 raw words (12751038 effective words) took 17.4s, 731878 effective words/s\n",
      "2025-10-15 11:08:20,171 : INFO : EPOCH 1 - PROGRESS: at 4.43% examples, 549091 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:21,171 : INFO : EPOCH 1 - PROGRESS: at 8.83% examples, 552979 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:22,183 : INFO : EPOCH 1 - PROGRESS: at 13.25% examples, 552466 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:23,185 : INFO : EPOCH 1 - PROGRESS: at 17.91% examples, 560631 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:24,190 : INFO : EPOCH 1 - PROGRESS: at 22.37% examples, 560864 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:25,203 : INFO : EPOCH 1 - PROGRESS: at 26.35% examples, 551015 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:26,211 : INFO : EPOCH 1 - PROGRESS: at 30.44% examples, 546128 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:27,218 : INFO : EPOCH 1 - PROGRESS: at 34.88% examples, 546947 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:28,224 : INFO : EPOCH 1 - PROGRESS: at 38.34% examples, 535225 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:29,227 : INFO : EPOCH 1 - PROGRESS: at 42.41% examples, 533754 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:30,413 : INFO : EPOCH 1 - PROGRESS: at 46.06% examples, 518812 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:31,415 : INFO : EPOCH 1 - PROGRESS: at 50.05% examples, 518489 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:32,428 : INFO : EPOCH 1 - PROGRESS: at 54.15% examples, 518215 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:33,428 : INFO : EPOCH 1 - PROGRESS: at 57.32% examples, 510451 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:34,442 : INFO : EPOCH 1 - PROGRESS: at 60.70% examples, 505097 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:35,456 : INFO : EPOCH 1 - PROGRESS: at 66.17% examples, 516680 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:36,464 : INFO : EPOCH 1 - PROGRESS: at 71.05% examples, 522571 words/s, in_qsize 6, out_qsize 1\n",
      "2025-10-15 11:08:37,488 : INFO : EPOCH 1 - PROGRESS: at 75.19% examples, 522233 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:38,514 : INFO : EPOCH 1 - PROGRESS: at 78.32% examples, 515257 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:39,541 : INFO : EPOCH 1 - PROGRESS: at 83.68% examples, 522576 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:40,557 : INFO : EPOCH 1 - PROGRESS: at 88.40% examples, 526197 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:41,578 : INFO : EPOCH 1 - PROGRESS: at 92.62% examples, 526181 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:42,589 : INFO : EPOCH 1 - PROGRESS: at 96.03% examples, 521812 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:43,547 : INFO : EPOCH 1: training on 17798268 raw words (12750053 effective words) took 24.4s, 522407 effective words/s\n",
      "2025-10-15 11:08:44,560 : INFO : EPOCH 2 - PROGRESS: at 4.21% examples, 533385 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:45,574 : INFO : EPOCH 2 - PROGRESS: at 8.50% examples, 534427 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:46,589 : INFO : EPOCH 2 - PROGRESS: at 12.62% examples, 527889 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:47,600 : INFO : EPOCH 2 - PROGRESS: at 17.30% examples, 541028 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:48,602 : INFO : EPOCH 2 - PROGRESS: at 21.80% examples, 546923 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:08:49,606 : INFO : EPOCH 2 - PROGRESS: at 26.62% examples, 557865 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:50,690 : INFO : EPOCH 2 - PROGRESS: at 31.20% examples, 554076 words/s, in_qsize 6, out_qsize 1\n",
      "2025-10-15 11:08:51,719 : INFO : EPOCH 2 - PROGRESS: at 35.73% examples, 554227 words/s, in_qsize 7, out_qsize 1\n",
      "2025-10-15 11:08:52,724 : INFO : EPOCH 2 - PROGRESS: at 40.58% examples, 561383 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:53,739 : INFO : EPOCH 2 - PROGRESS: at 45.00% examples, 560928 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:54,745 : INFO : EPOCH 2 - PROGRESS: at 49.77% examples, 565432 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:55,900 : INFO : EPOCH 2 - PROGRESS: at 54.10% examples, 557145 words/s, in_qsize 8, out_qsize 3\n",
      "2025-10-15 11:08:56,903 : INFO : EPOCH 2 - PROGRESS: at 58.01% examples, 553286 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:57,919 : INFO : EPOCH 2 - PROGRESS: at 62.37% examples, 553065 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:58,923 : INFO : EPOCH 2 - PROGRESS: at 66.63% examples, 552353 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:08:59,926 : INFO : EPOCH 2 - PROGRESS: at 71.16% examples, 553851 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:00,934 : INFO : EPOCH 2 - PROGRESS: at 75.20% examples, 551402 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:01,934 : INFO : EPOCH 2 - PROGRESS: at 79.63% examples, 552128 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:02,962 : INFO : EPOCH 2 - PROGRESS: at 84.06% examples, 552013 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:03,980 : INFO : EPOCH 2 - PROGRESS: at 88.29% examples, 551153 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:04,997 : INFO : EPOCH 2 - PROGRESS: at 93.01% examples, 553041 words/s, in_qsize 6, out_qsize 1\n",
      "2025-10-15 11:09:06,001 : INFO : EPOCH 2 - PROGRESS: at 97.57% examples, 554111 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:06,539 : INFO : EPOCH 2: training on 17798268 raw words (12752621 effective words) took 23.0s, 554790 effective words/s\n",
      "2025-10-15 11:09:07,549 : INFO : EPOCH 3 - PROGRESS: at 4.37% examples, 555169 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:08,552 : INFO : EPOCH 3 - PROGRESS: at 8.67% examples, 548217 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:09,573 : INFO : EPOCH 3 - PROGRESS: at 13.14% examples, 550237 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:10,575 : INFO : EPOCH 3 - PROGRESS: at 18.30% examples, 574917 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:11,597 : INFO : EPOCH 3 - PROGRESS: at 23.43% examples, 587371 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:12,597 : INFO : EPOCH 3 - PROGRESS: at 28.53% examples, 597713 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:13,612 : INFO : EPOCH 3 - PROGRESS: at 33.66% examples, 602845 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:14,631 : INFO : EPOCH 3 - PROGRESS: at 38.68% examples, 606553 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:15,630 : INFO : EPOCH 3 - PROGRESS: at 43.75% examples, 611353 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:16,651 : INFO : EPOCH 3 - PROGRESS: at 48.72% examples, 612595 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:17,664 : INFO : EPOCH 3 - PROGRESS: at 53.72% examples, 614040 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:18,664 : INFO : EPOCH 3 - PROGRESS: at 58.51% examples, 614636 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:19,673 : INFO : EPOCH 3 - PROGRESS: at 63.39% examples, 614763 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:20,686 : INFO : EPOCH 3 - PROGRESS: at 68.32% examples, 615269 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:21,695 : INFO : EPOCH 3 - PROGRESS: at 73.39% examples, 617231 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:22,707 : INFO : EPOCH 3 - PROGRESS: at 78.43% examples, 618429 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:23,714 : INFO : EPOCH 3 - PROGRESS: at 83.45% examples, 619216 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:24,718 : INFO : EPOCH 3 - PROGRESS: at 88.52% examples, 620817 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:25,724 : INFO : EPOCH 3 - PROGRESS: at 93.40% examples, 620700 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:26,729 : INFO : EPOCH 3 - PROGRESS: at 96.58% examples, 609663 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:27,666 : INFO : EPOCH 3: training on 17798268 raw words (12748568 effective words) took 21.1s, 603592 effective words/s\n",
      "2025-10-15 11:09:28,695 : INFO : EPOCH 4 - PROGRESS: at 5.81% examples, 725062 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:29,699 : INFO : EPOCH 4 - PROGRESS: at 9.98% examples, 623633 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:30,714 : INFO : EPOCH 4 - PROGRESS: at 12.97% examples, 540381 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:31,715 : INFO : EPOCH 4 - PROGRESS: at 17.91% examples, 560460 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:32,719 : INFO : EPOCH 4 - PROGRESS: at 22.76% examples, 570755 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:33,734 : INFO : EPOCH 4 - PROGRESS: at 27.74% examples, 580066 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:34,735 : INFO : EPOCH 4 - PROGRESS: at 32.69% examples, 585801 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:35,737 : INFO : EPOCH 4 - PROGRESS: at 37.51% examples, 589265 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:36,749 : INFO : EPOCH 4 - PROGRESS: at 42.24% examples, 590639 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:37,761 : INFO : EPOCH 4 - PROGRESS: at 47.10% examples, 593037 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:38,762 : INFO : EPOCH 4 - PROGRESS: at 51.87% examples, 594361 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:39,767 : INFO : EPOCH 4 - PROGRESS: at 56.21% examples, 591047 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:40,770 : INFO : EPOCH 4 - PROGRESS: at 60.47% examples, 587741 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:41,771 : INFO : EPOCH 4 - PROGRESS: at 64.56% examples, 583047 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:42,771 : INFO : EPOCH 4 - PROGRESS: at 68.38% examples, 576603 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:43,773 : INFO : EPOCH 4 - PROGRESS: at 72.44% examples, 573158 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-15 11:09:44,790 : INFO : EPOCH 4 - PROGRESS: at 76.66% examples, 570471 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:45,793 : INFO : EPOCH 4 - PROGRESS: at 80.93% examples, 568888 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:46,802 : INFO : EPOCH 4 - PROGRESS: at 84.83% examples, 565027 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:47,822 : INFO : EPOCH 4 - PROGRESS: at 89.04% examples, 563092 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:48,825 : INFO : EPOCH 4 - PROGRESS: at 93.07% examples, 560710 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:49,850 : INFO : EPOCH 4 - PROGRESS: at 97.08% examples, 557700 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-15 11:09:50,584 : INFO : EPOCH 4: training on 17798268 raw words (12749120 effective words) took 22.9s, 556374 effective words/s\n",
      "2025-10-15 11:09:50,585 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991340 raw words (63751400 effective words) took 108.9s, 585532 effective words/s', 'datetime': '2025-10-15T11:09:50.585426', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-10-15 11:09:50,586 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2025-10-15T11:09:50.586426', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-10-15 11:09:50,591 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-10-15T11:09:50.591946', 'gensim': '4.3.3', 'python': '3.9.23 (main, Jun  5 2025, 13:25:08) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-10-15 11:09:50,592 : INFO : not storing attribute cum_table\n",
      "2025-10-15 11:09:50,644 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 三、模型效果测试",
   "id": "b204bd24dc4f4d5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T03:16:45.954523Z",
     "start_time": "2025-10-15T03:16:45.935398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 找出语义最不匹配的单词\n",
    "# 预期输出“kitchen”\n",
    "print(model.wv.doesnt_match(\"man woman child kitchen\".split()))\n",
    "# 预期输出“berlin”\n",
    "print(model.wv.doesnt_match(\"france england germany berlin\".split()))\n",
    "# 预期输出“austria”\n",
    "print(model.wv.doesnt_match(\"paris berlin london austria\".split()))"
   ],
   "id": "16723ffbfa71e8af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n",
      "austria\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T03:20:21.473846Z",
     "start_time": "2025-10-15T03:20:21.451086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试语义相似性\n",
    "print(model.wv.most_similar(\"man\"))\n",
    "print(model.wv.most_similar(\"awful\"))"
   ],
   "id": "e4179e84d74af7a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.597867488861084), ('lady', 0.5709412693977356), ('lad', 0.5448217988014221), ('monk', 0.5355395674705505), ('soldier', 0.5192042589187622), ('farmer', 0.5134194493293762), ('businessman', 0.5132336020469666), ('guy', 0.510189950466156), ('men', 0.5049229264259338), ('millionaire', 0.5029137134552002)]\n",
      "[('terrible', 0.7625946998596191), ('horrible', 0.7238616347312927), ('atrocious', 0.7134142518043518), ('dreadful', 0.7055865526199341), ('abysmal', 0.6959513425827026), ('horrendous', 0.6813711524009705), ('appalling', 0.6618828773498535), ('horrid', 0.6542140245437622), ('lousy', 0.617429792881012), ('laughable', 0.6039625406265259)]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 四、模型应用",
   "id": "8919d6d2728b2773"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. 加载模型",
   "id": "c36373c1b340f5c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:24:03.381688Z",
     "start_time": "2025-10-15T06:24:02.816075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "# 加载模型\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "# wv.vectors中保存着每个词的特征向量\n",
    "print(type(model.wv.vectors))\n",
    "print(model.wv.vectors.shape)\n",
    "# 查看词“flower”的向量表示\n",
    "print(model.wv[\"flower\"])"
   ],
   "id": "24250edc44e442fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16490, 300)\n",
      "[-1.19871274e-01  5.72454214e-01 -8.74414593e-02 -8.96268617e-03\n",
      "  2.58887142e-01  2.08539784e-01  8.73589069e-02  1.18361644e-01\n",
      "  3.18020821e-01 -1.92725658e-01 -1.57137558e-01  3.02065950e-04\n",
      "  3.02821137e-02  2.56524414e-01 -9.65133682e-02  8.24111402e-02\n",
      " -1.54083237e-01 -4.33911949e-01  2.14469917e-02  1.19003229e-01\n",
      " -1.69126526e-01  9.99797136e-02  2.73753405e-01  2.07810163e-01\n",
      "  6.10518694e-01 -1.43236015e-02 -5.21949649e-01  1.14180721e-01\n",
      "  1.56046227e-01  9.39961821e-02  2.79568464e-01 -1.41498566e-01\n",
      " -9.50638875e-02 -3.09604704e-01 -3.21064085e-01 -1.11517929e-01\n",
      "  4.28073317e-01 -2.57438987e-01 -1.33117929e-01  3.76034528e-01\n",
      " -2.19613284e-01  2.04722106e-01  3.61238539e-01 -2.50527591e-01\n",
      " -3.57289404e-01  4.88126129e-01  1.85666725e-01  4.15443391e-01\n",
      " -4.11319762e-01  4.53914672e-01  6.38770103e-01  8.83016810e-02\n",
      " -1.62773967e-01  1.17148414e-01 -2.22932115e-01  4.46077794e-01\n",
      "  1.67725056e-01  1.65523648e-01 -3.94457310e-01 -1.31961256e-01\n",
      " -5.93961954e-01  5.80159009e-01 -3.22986431e-02 -1.08288713e-01\n",
      "  2.41729263e-02 -7.43144751e-02 -4.26654696e-01  2.12057486e-01\n",
      "  2.77392685e-01 -3.76094162e-01 -2.23784968e-01  5.83340637e-02\n",
      "  4.71810728e-01 -4.00683880e-02 -1.33805752e-01 -4.66439426e-02\n",
      "  2.52345018e-02  1.83797568e-01 -2.11901501e-01  2.14467049e-01\n",
      "  1.23790443e-01  2.76175559e-01  1.76309958e-01  6.50595307e-01\n",
      " -7.52379075e-02  4.21034247e-01 -3.58594537e-01  2.34811485e-01\n",
      "  1.89323604e-01  1.00430287e-01  5.44294044e-02  1.69331983e-01\n",
      " -4.56791639e-01  2.20715076e-01  2.35911146e-01  2.36454383e-01\n",
      "  2.79310584e-01 -8.55949223e-02 -7.57274091e-01  5.86688459e-01\n",
      "  3.01840365e-01  2.65598655e-01  2.84787774e-01 -2.15729222e-01\n",
      "  8.01231086e-01  2.00876035e-03 -1.63555920e-01  1.01207636e-01\n",
      "  2.30557978e-01 -1.72548890e-01 -1.68190494e-01  6.19153678e-02\n",
      " -1.79947063e-01  3.33415717e-01  4.76622581e-01  1.88905343e-01\n",
      "  3.85873228e-01  3.61392200e-01  1.31996244e-01 -3.16861689e-01\n",
      "  2.24912032e-01 -3.07873636e-02  1.29567146e-01 -1.30842421e-02\n",
      " -2.97680378e-01  1.73694625e-01  6.51495978e-02 -2.33549252e-01\n",
      "  6.72228858e-02  6.29883930e-02 -2.35305130e-01  1.15670227e-01\n",
      "  2.65271123e-02 -3.45372677e-01 -1.72480896e-01 -4.69565302e-01\n",
      " -7.53854737e-02  3.50937068e-01 -7.61782154e-02  4.22943592e-01\n",
      " -3.16251278e-01 -3.69607627e-01 -1.82946667e-03  1.90411061e-01\n",
      "  1.34518221e-02  1.98814794e-02 -1.57632351e-01 -2.32569560e-01\n",
      "  2.55714953e-01  7.13277340e-01  3.29731792e-01 -2.90890247e-01\n",
      " -5.29644012e-01 -8.57345685e-02 -1.07207306e-01 -2.50876695e-01\n",
      " -1.65969953e-01 -3.32668684e-02 -3.46405625e-01  5.00208557e-01\n",
      "  3.11941713e-01 -1.27014324e-01 -4.40331966e-01  1.18251666e-01\n",
      "  4.97860201e-02  1.15370281e-01  1.75633386e-01 -1.25589937e-01\n",
      "  4.72288169e-02 -1.07987516e-01  3.07221830e-01  8.39297324e-02\n",
      " -2.16493115e-01 -1.82986423e-01 -6.11799061e-01 -5.55511475e-01\n",
      "  6.64852858e-01 -1.00243852e-01 -3.34355116e-01 -3.87977123e-01\n",
      "  7.21718222e-02  1.38292640e-01 -3.05130154e-01 -8.14566851e-01\n",
      " -1.42360538e-01  3.17648500e-01  1.47527561e-01  6.05303288e-01\n",
      "  3.10422331e-01 -2.27777407e-01 -2.67936796e-01 -1.61514580e-01\n",
      " -4.10514921e-01  2.23348722e-01 -1.76814094e-01 -2.20980614e-01\n",
      " -1.36471102e-02 -6.81410581e-02  2.51137257e-01 -4.24136251e-01\n",
      " -2.81879365e-01  3.60191137e-01  4.30799574e-01 -4.60574836e-01\n",
      " -6.40558600e-01  6.24188520e-02 -2.30623707e-01  1.49420962e-01\n",
      " -1.47584841e-01 -3.66779566e-02 -7.23153204e-02 -8.91387239e-02\n",
      "  1.31565318e-01 -1.39916375e-01 -1.30451754e-01  4.67604965e-01\n",
      " -9.57231820e-02 -3.76285493e-01  3.36419672e-01 -2.85888195e-01\n",
      "  4.74004358e-01  7.25557581e-02 -4.74642366e-02  3.04748654e-01\n",
      " -7.27170333e-02  3.51666519e-03 -8.47196728e-02 -3.86818573e-02\n",
      "  6.72533549e-03 -3.01247150e-01  2.88991667e-02  7.67233223e-02\n",
      " -4.43569094e-01  4.19491082e-01  9.03498307e-02  8.33085328e-02\n",
      " -1.66615367e-01  7.78218657e-02  2.94136852e-01 -4.40448433e-01\n",
      "  4.89999324e-01 -4.52567071e-01 -1.97759822e-01 -1.14145920e-01\n",
      "  4.51421626e-02  5.71918227e-02 -4.24133427e-02  1.37193324e-02\n",
      " -1.85195163e-01  3.68833393e-01 -3.77781183e-01  2.00903967e-01\n",
      "  1.06593587e-01 -5.31740226e-02 -9.78364050e-02 -6.85967922e-01\n",
      "  1.42178044e-01 -1.76525354e-01 -4.36502546e-01  2.56457537e-01\n",
      "  2.10171819e-01 -4.14559364e-01  8.61107633e-02 -1.03581980e-01\n",
      " -1.86060727e-01 -1.59670077e-02  4.23567325e-01  3.14621449e-01\n",
      "  1.95591047e-01  1.11385837e-01 -4.49160397e-01  3.68698299e-01\n",
      " -1.23460978e-01 -1.67759374e-01  5.28364420e-01 -4.82196832e-04\n",
      "  4.42557365e-01 -3.05525474e-02 -4.68243688e-01  3.78510952e-01\n",
      "  5.98460101e-02  1.28713638e-01 -3.91468890e-02  2.79204291e-03\n",
      "  4.84118879e-01 -1.13250718e-01 -2.83348769e-01  6.83943331e-01\n",
      "  2.24424616e-01  5.28510690e-01 -5.79191327e-01  3.08267206e-01\n",
      "  5.66846609e-01  1.20556215e-02  1.51309103e-01  5.23042917e-01\n",
      " -6.02607906e-01 -8.95224139e-02  3.70871574e-01  3.09247598e-02]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 向量平均法\n",
    "使用 Word2Vec 模型将评论转换为数值向量(300维)，然后用随机森林进行情感分析"
   ],
   "id": "c8acfd56ba25998c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:40:01.875760Z",
     "start_time": "2025-10-15T06:38:55.143963Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25880\\1221155664.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(raw_review, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "import numpy as np\n",
    "# 计算一个词列表中所有词向量的均值\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros(num_features, dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # 模型词汇表中的单词集合\n",
    "    word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in word_set:\n",
    "            nwords += 1\n",
    "            featureVec = np.add(featureVec, model.wv[word])\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# 对每个评论调用makeFeatureVec,生成二维向量矩阵\n",
    "def getAvgFeatureVec(reviews, model, num_features):\n",
    "    # reviews 是一个列表，元素为词列表\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter += 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "# 计算平均向量\n",
    "num_features = 300\n",
    "clean_train_reviews = []        # 记录每条评论转为词列表的结果\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVec(clean_train_reviews, model, num_features)\n",
    "# 计算测试集评论的平均特征向量\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVec(clean_test_reviews, model, num_features)\n"
   ],
   "id": "f27d317384c9f813"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:40:55.333803Z",
     "start_time": "2025-10-15T06:40:07.219783Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "# 构造随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 创建随机森林\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "# 依据word模型训练\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "# 模型预测\n",
    "result = forest.predict(testDataVecs)\n",
    "# 将模型预测结果保存到csv文件中\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "\n",
    "# 将输出结果保存在csv文件中\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)"
   ],
   "id": "69520ca3d25ec74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 聚类法 k-means",
   "id": "3cd3daf7654deb37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:45:20.836557Z",
     "start_time": "2025-10-15T06:43:22.201563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = model.wv.vectors\n",
    "# 设置参数k的取值为词汇表大小的五分之一，每个簇中约有5个词\n",
    "num_clusters = int(word_vectors.shape[0]/5)\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "# 每个词向量所属簇的编号\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "# 使用字典将每个词与其所属簇编号映射起来\n",
    "word_centroid_map = dict(zip(model.wv.index_to_key, idx))"
   ],
   "id": "297cc41216e27ef4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:46:26.826109Z",
     "start_time": "2025-10-15T06:46:26.806085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 输出十个簇的结果\n",
    "for cluster in range(10):\n",
    "    print(\"Cluster %d of %d\" % (cluster, num_clusters))\n",
    "    words = []\n",
    "    for word, word_cluster in word_centroid_map.items():\n",
    "        if word_cluster == cluster:\n",
    "            words.append(word)\n",
    "    print(words)"
   ],
   "id": "c544806620c4b1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 of 3298\n",
      "['franco']\n",
      "Cluster 1 of 3298\n",
      "['magazine', 'reunion', 'nyc', 'brooklyn', 'attend', 'boston', 'studying', 'gallery', 'attended', 'graduate', 'attending', 'prom', 'boarding', 'elementary', 'annual', 'freshman', 'grad']\n",
      "Cluster 2 of 3298\n",
      "['fest']\n",
      "Cluster 3 of 3298\n",
      "['harrison', 'wallace']\n",
      "Cluster 4 of 3298\n",
      "['ridiculous', 'laughable', 'absurd', 'ludicrous']\n",
      "Cluster 5 of 3298\n",
      "['pity', 'producing', 'flop', 'showcase', 'caliber', 'contribution', 'breakthrough']\n",
      "Cluster 6 of 3298\n",
      "['sandra', 'dee', 'jolie', 'angelina', 'bullock']\n",
      "Cluster 7 of 3298\n",
      "['doomed', 'distant', 'raising', 'shared', 'challenges', 'sharing', 'eternal', 'immediate', 'continuing', 'dilemma', 'mutual', 'sadako', 'youthful', 'questioning', 'experiencing', 'trials', 'realization', 'enduring', 'eventual', 'sacrifices', 'embrace', 'gandhi', 'fragile', 'impending', 'keane', 'asoka', 'devotion', 'torment', 'maturity', 'endured', 'denial', 'forgiveness', 'fulfilling', 'marital', 'temptation', 'sorrow', 'existential', 'expressing', 'hunger', 'bonding', 'intimacy', 'healing', 'dedication', 'kindness', 'optimism', 'heroism', 'burden', 'alienation', 'infidelity', 'deepest', 'friendships', 'oneself', 'alcoholism', 'joys', 'sacrificing', 'sibling', 'gaining', 'bravery', 'palpable', 'hardships', 'rejection', 'separation', 'tribulations', 'yearning', 'coping', 'heartbreak', 'fulfillment', 'purity', 'hopelessness', 'uncertainty', 'conflicting', 'innate', 'adversity', 'hardship', 'gradual', 'adolescence', 'unrequited', 'reconciliation', 'companionship']\n",
      "Cluster 8 of 3298\n",
      "['therapy', 'mice', 'bashing', 'whining', 'perverted', 'cocaine', 'sixteen', 'anonymous', 'witchcraft', 'ensue', 'marijuana', 'hallucinations', 'prospect', 'casually', 'fetish', 'tease', 'gossip', 'dope', 'starving', 'violently', 'behaving', 'calendar', 'trendy', 'kinky', 'cursing', 'jerks', 'trading', 'flock', 'heterosexual', 'classroom', 'pranks', 'meetings', 'bullying', 'bastards', 'shenanigans', 'rituals', 'lords', 'shootings', 'cannibalism', 'starved', 'hookers', 'pets', 'practicing', 'partying', 'anal', 'mutilated', 'cows', 'oral', 'whores', 'verbally', 'strippers', 'showers', 'psychos', 'innocents', 'chant', 'addicts', 'brats', 'inbred', 'hugging', 'jocks', 'cheerleaders', 'promiscuous', 'voyeuristic', 'farting', 'butchering', 'beatings', 'degenerate', 'nubile', 'undress', 'kitten', 'leering', 'bigoted', 'whipping', 'mischief', 'acquaintances', 'routinely', 'forsaken', 'hijinks', 'professors', 'pesky', 'blondes', 'schoolgirl', 'removal', 'pedophilia', 'encountering', 'curses', 'tattoos', 'fiends', 'baring', 'virgins', 'wongo', 'orphans', 'priestess', 'adolescents', 'psychopaths', 'commune', 'innuendos', 'snobs', 'addled', 'outcasts', 'pimps', 'opium', 'boobies', 'delinquent', 'bimbos', 'dwellers', 'maids', 'systematically', 'jive', 'bred', 'harem', 'pubescent', 'slaughtering', 'weirdos', 'parasites', 'protests', 'crushes', 'enslaved', 'pairs', 'necrophilia', 'offenders', 'bestiality', 'perverts', 'insatiable', 'lunatics']\n",
      "Cluster 9 of 3298\n",
      "['wacky', 'zany', 'nutty']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:46:34.868395Z",
     "start_time": "2025-10-15T06:46:34.858761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将词列表转为一个向量，维度为簇的个数，bag_of_centroids[index]表示本评论中属于编号“index”的簇的词个数\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    # 确定簇的数量\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            # 记录词出现次数\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ],
   "id": "f8fe43e85c523c08",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:46:57.174700Z",
     "start_time": "2025-10-15T06:46:36.904036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 为训练集生成向量\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "# 为测试集生成向量\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ],
   "id": "9efe8a12c759ae66",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:48:28.127066Z",
     "start_time": "2025-10-15T06:47:30.433827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造随机森林\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "# 模型训练\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])\n",
    "# 模型预测\n",
    "result = forest.predict(test_centroids)\n",
    "# 预测结果保存\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)"
   ],
   "id": "e48d6f70d28fb8bd",
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
